
# Database Migration Strategy (Alembic with FastAPI & SQLAlchemy)

## 1. Overview

*   **Purpose**: To establish clear procedures, best practices, and tools for managing database schema changes (migrations) using **Alembic** with **SQLAlchemy** for the FastAPI-based ERP project. This ensures data integrity, minimizes deployment risks, and facilitates collaboration.
*   **Scope**: Covers Alembic setup, migration script generation (including autogeneration and manual authoring), review processes, application of migrations, conflict resolution, testing, deployment of schema changes, data migrations, and general considerations for database health related to schema evolution. Specific operational backup/restore procedures beyond migration safety are covered in `database_postgresql.md`.
*   **Goal**: Predictable, reliable, and low-risk database schema evolution throughout the project lifecycle.

## 2. Core Principles

*   **Consistency**: All developers follow the same Alembic migration workflow.
*   **Atomicity**: Migrations should represent logical, atomic schema changes where possible. Avoid overly large, multi-purpose migrations.
*   **Reversibility (Best Effort)**: Migration scripts should implement both `upgrade()` and `downgrade()` functions. Downgrade paths must be tested locally if intended for use in any recovery scenario.
*   **Idempotency**: Applying migrations (`alembic upgrade head`) multiple times has the same effect as applying them once. Alembic handles this for schema changes.
*   **Thorough Review & Testing**: Alembic-generated scripts, especially autogenerated ones, **must be critically reviewed and tested** locally and in staging environments before production deployment.
*   **Version Control**: Migration script files (`.py` files in `alembic/versions/`) are code and must be committed to version control (Git).
*   **Communication**: Teams must communicate proactively about schema changes.
*   **Backup First**: Mandatory verified database backups (as per `database_postgresql.md` policies) before applying significant migrations in production.

## 3. Alembic Setup & Configuration (Strategic Overview)
    * Installation: `alembic` in `requirements/base.txt`.
    * Initialization: `alembic init alembic`.
    * `alembic.ini` Configuration: `sqlalchemy.url` pointing to database (via env var).
    * `alembic/env.py` Configuration:
        * Import `Base.metadata` from `app.core.db`.
        * Set `target_metadata = Base.metadata`.
        * Configure for asynchronous operations for online mode.
        * Implement or ensure naming conventions for constraints are used in models to aid autogenerate.

## 4. Migration Workflow & Best Practices (Strategic Overview)
    * Development Environment Workflow: Model Changes -> Generate Alembic Script -> Meticulously Review & Edit Script -> Apply Locally -> Test Application & Migration -> Commit.
    * Version Control (Git): Feature branches, frequent rebasing, clear communication for resolving divergent histories (multiple heads), PR reviews include migration scripts.
    * Staging & Production Deployment: Follow procedures in `deployment_strategy_and_ci_cd.md`, including pre-migration backups.

## 5. Handling Specific Migration Scenarios (Strategic Overview)
    * Adding Non-Nullable Columns (multi-step or with server_default).
    * Renaming Tables/Columns (careful review of `op.rename_table`, `op.alter_column`).
    * Data Migrations (using `op.execute()` or `op.bulk_insert()`; test thoroughly).
    * Circular Dependencies between tables (two-phase migrations, `ForeignKey(use_alter=True)`, or `DEFERRABLE` constraints).
    * Adding Indexes Concurrently on large PostgreSQL tables (using `op.execute('CREATE INDEX CONCURRENTLY ...')` within an `autocommit_block`).

## 6. Database Management & Maintenance (Related to Schema Evolution)
    * While comprehensive database backup, recovery, and availability strategies are detailed in `database_postgresql.md` (and may involve a dedicated DB Operations API/module for self-managed instances or reliance on cloud provider services), migrations themselves necessitate specific precautions:
        * **Pre-Migration Backups:** Always perform a full, verified backup before running `alembic upgrade` in production.
        * **Monitoring During Migration:** Observe database performance and logs during migration execution.
        * **Rollback of Migrations:** Downgrading migrations (`alembic downgrade`) in production is high-risk and should generally be avoided in favor of restoring from backup or rolling forward with a fix-up migration. Thorough local testing of `downgrade()` functions is required if they are ever considered for use.
    * General PostgreSQL maintenance (vacuuming, analyze, index maintenance, etc.) is crucial for performance but is an operational concern detailed in `database_postgresql.md`.

## 7. Tooling (Strategic Overview)
    * Alembic CLI, SQLAlchemy, Git, Database GUI.

## 8. General Setup Implementation Details (for Alembic)

    This section details the one-time setup and core configurations for Alembic.

    ### 8.1. Library Installation
    *   Ensure `alembic>=1.11.0,<1.12.0` (or latest stable) is present in `requirements/base.txt`.
    *   Ensure `sqlalchemy[asyncio]` and `asyncpg` (or relevant async DB driver) are also in `requirements/base.txt`.

    ### 8.2. Initializing Alembic
    1.  From the project root directory, execute:
        ```bash
        docker-compose run --rm api alembic init alembic
        ```
        (Assuming `api` is your FastAPI application service in `docker-compose.yml` and it has Python/Alembic installed). This creates the `alembic/` directory and `alembic.ini`.

    ### 8.3. Configuring `alembic.ini`
    *   Open `alembic.ini`.
    *   Modify the `sqlalchemy.url` line to load from the same environment variable used by your Pydantic `Settings` for the main application:
        ```ini
        # alembic.ini
        sqlalchemy.url = %(DATABASE_URL)s
        ```
        *Ensure `DATABASE_URL` is exported to the environment when running `alembic` commands.*

    ### 8.4. Configuring `alembic/env.py`
    *   Modify `alembic/env.py` for SQLAlchemy model awareness and async operation.
        ```python
        # alembic/env.py
        import asyncio
        from logging.config import fileConfig

        from sqlalchemy.ext.asyncio import create_async_engine
        from sqlalchemy import pool

        from alembic import context

        # Add project root to sys.path to allow importing app modules
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

        from app.core.db import Base  # Your SQLAlchemy declarative base
        from app.core.config import settings as app_settings # Your Pydantic settings

        config = context.config

        if config.config_file_name is not None:
            fileConfig(config.config_file_name)

        target_metadata = Base.metadata # Makes Alembic aware of your models for autogenerate

        # Naming convention for autogenerated constraints (recommended for consistency)
        # Ensure your SQLAlchemy models also use explicit naming for critical constraints.
        convention = {
            "ix": "ix_%(table_name)s_%(column_0_label)s",
            "uq": "uq_%(table_name)s_%(column_0_name)s",
            "ck": "ck_%(table_name)s_%(constraint_name)s",
            "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
            "pk": "pk_%(table_name)s"
        }

        def run_migrations_offline() -> None:
            db_url_str = app_settings.DATABASE_URL.render_as_string(hide_password=False) \
                if hasattr(app_settings.DATABASE_URL, 'render_as_string') \
                else str(app_settings.DATABASE_URL)
            # For offline mode, ensure we use a URL string that a sync engine can handle
            # if your main URL is async only. Usually, for offline, it just generates SQL.
            # If your DATABASE_URL uses 'postgresql+asyncpg', replace with 'postgresql' for offline.
            offline_db_url = db_url_str.replace("+asyncpg", "")

            context.configure(
                url=offline_db_url,
                target_metadata=target_metadata,
                literal_binds=True,
                dialect_opts={"paramstyle": "named"},
                render_as_batch=True,
                naming_convention=convention
            )
            with context.begin_transaction():
                context.run_migrations()

        def do_run_migrations(connection):
            context.configure(
                connection=connection,
                target_metadata=target_metadata,
                render_as_batch=True,
                naming_convention=convention,
                compare_type=True,
                compare_server_default=True
            )
            with context.begin_transaction():
                context.run_migrations()

        async def run_migrations_online() -> None:
            db_url_str = app_settings.DATABASE_URL.render_as_string(hide_password=False) \
                if hasattr(app_settings.DATABASE_URL, 'render_as_string') \
                else str(app_settings.DATABASE_URL)

            connectable = create_async_engine(db_url_str, poolclass=pool.NullPool)
            async with connectable.connect() as connection:
                await connection.run_sync(do_run_migrations)
            await connectable.dispose()

        if context.is_offline_mode():
            run_migrations_offline()
        else:
            asyncio.run(run_migrations_online())
        ```

    ### 8.5. Initial Migration (First Schema Creation)
    *   After defining initial SQLAlchemy models and configuring Alembic:
        ```bash
        docker-compose run --rm api alembic revision --autogenerate -m "create_initial_tables"
        ```
    *   Carefully review the generated script in `alembic/versions/`.
    *   Apply it to your (empty) development database:
        ```bash
        docker-compose run --rm api alembic upgrade head
        ```

## 9. Integration & Usage Patterns (Developer Workflow for Schema Migrations)

    This section outlines the typical workflow for developers when making schema changes, reinforcing best practices.

    ### 9.1. Making Model Changes
    *   Modify SQLAlchemy models in `app/.../models.py`. Strive for small, logical changes per migration.

    ### 9.2. Generating a New Migration Script
    *   Use `docker-compose run --rm api alembic revision --autogenerate -m "your_descriptive_message"`.

    ### 9.3. Reviewing and Editing the Migration Script (MANDATORY)
    *   **Open and meticulously review** the Python script in `alembic/versions/`.
    *   Verify all `op.*` calls, data types, nullability, defaults, foreign keys (with `ondelete`/`onupdate` and names), indexes, and unique constraints.
    *   Ensure the `downgrade()` function is correct and safe if reversibility is intended.
    *   Assess for potential locking or performance issues on large tables and plan accordingly (e.g., use concurrent index creation patterns if needed).

    ### 9.4. Applying and Testing Locally
    1.  Apply: `docker-compose run --rm api alembic upgrade head`.
    2.  Run Pytest suite.
    3.  Manually test affected features.
    4.  Optionally, test `downgrade` locally: `... alembic downgrade -1` then `... alembic upgrade head`.

    ### 9.5. Committing Changes
    *   Commit SQLAlchemy model changes AND the corresponding Alembic migration script(s) together in a single Git commit.

    ### 9.6. Handling Data Migrations
    *   Generate an empty script: `... alembic revision -m "data_migration_description"`.
    *   Implement data changes in `upgrade()` using `op.execute("SQL...")` or `op.bulk_insert()`. Ensure corresponding `downgrade()` if feasible. Test extensively.

    ### 9.7. Resolving Migration Conflicts
    *   If divergent histories occur (multiple heads):
        *   Inspect with `alembic history`.
        *   Options:
            *   **Merge Heads:** `alembic merge <rev1> <rev2> -m "merge_conflicting_branches"`. Review the generated merge script.
            *   **Rebase & Regenerate (often preferred for linearity):** Rebase your feature branch, resolve model code conflicts, delete your local (unpushed) migration files, then regenerate a *new* migration script from the merged models. This keeps history linear.

